# -*- coding: utf-8 -*-
"""Copy of LRCN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SgCAzy-OdBxggjMu5NwBT9x6WAmGvRvR

#Model Training
"""
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import time
import tensorflow as tf
from collections import deque
# import matplotlib.pyplot as plt
# from google.colab import drive
# from IPython.display import Image
#from moviepy.editor import *
# %matplotlib inline
 
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder
# from tensorflow.keras.layers import *
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.utils import to_categorical
# from tensorflow.keras.callbacks import EarlyStopping
# from tensorflow.keras.utils import plot_model
from tensorflow import keras
from datetime import datetime
import django
django.setup()
from Violations.models import Activity, Policy, Violations
from Violations.serializers import ActivitySerializer, PolicySerializer, ViolationsSerializer

model = keras.models.load_model("convlstm_model___Date_Time_2022_11_29__08_32_02___Loss_0.5234410762786865___Accuracy_0.7096773982048035.h5")

# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64
# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
# Reading names
# <===============
# ==========================ADD YOUR DRIVE CLIPPED FOLDERS PATH IN HERE ========================================>

# from google.colab.patches import cv2_imshow
# Algorithm
# Policy Checker Main
# take input from camera of SEQUENCE_LENGTH
# get the zone
# attach an auto generated id and date,time
# pass the input to face recog and action recog
# combine the outputs from both models
# send the data to policy checker.
# the output of policy checker is stored into the data base.
def all_activities():
    activity = Activity.objects.all()
    serializer = ActivitySerializer(activity, many=True)
    ALL_CLASSES = []
    for el in serializer.data:
        ALL_CLASSES.append(el['actname'])
    return ALL_CLASSES

def get_activities():
    activity = Activity.objects.filter(enabled=True)
    serializer = ActivitySerializer(activity, many=True)
    CLASSES_LIST = []
    for el in serializer.data:
        CLASSES_LIST.append(el['actname'])
    return CLASSES_LIST

def get_policy():
    policy = Policy.objects.filter(enabled=True)
    serializer = PolicySerializer(policy, many=True)
    return serializer.data


def convert_time_minutes(time):
    x = time.split(':')
    minutes = int(x[0])*60 + int(x[1])
    return minutes
          
def attach_time():
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    return convert_time_minutes(current_time)
  
def system_loop(SEQUENCE_LENGTH):
    input_video_file_path = f'predict_video/T1Gun.mp4' 
    vidcap = cv2.VideoCapture(input_video_file_path)

    activity_data = dict()
    input_id = 0
    count = 0
    success = True
    #frames_queue = deque(maxlen = SEQUENCE_LENGTH)
    frames_queue = []
    violations = []
    temp_violation = False
    policy_time_check = time.time()
    violation_time_check = time.time()

    prev_violation = False
    ALL_CLASSES = all_activities()
    POLICIES = get_policy()
    CLASSES_LIST = get_activities()

    while vidcap.isOpened():

        if time.time() - policy_time_check > 20:
            print('Reloading Policies and Activities')
            policy_time_check = time.time()
            POLICIES = get_policy()
            CLASSES_LIST = get_activities()

        success,image = vidcap.read()
        if not success:
            break
    
        resized_frame = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))
        normalized_frame = resized_frame / 255
        frames_queue.append(normalized_frame)
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]
            # print(predicted_labels_probabilities)
            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)
            # print(predicted_label)

            # Get the class name using the retrieved index.
            # Checking that the detected activty is enabled
            if ALL_CLASSES[predicted_label] in CLASSES_LIST:
                predicted_class_name = ALL_CLASSES[predicted_label]
            else:
                continue

            # attaching the id and time (minutes) for the stream of input
            activity_data['id'] = input_id
            activity_data['activity'] = predicted_class_name
            activity_data['person'] = None
            activity_data['zone'] = 'A' # this will be changed in the future
            activity_data['time'] = datetime.now().strftime("%H:%M:%S")
            # print(activity_data)
            violation = check_policy(POLICIES, activity_data)
            print("Activity:")
            print(activity_data)
            print("Violation:")
            print(violation)

            if violation != False:

                if temp_violation == False:
                    # first activity or first after complition of an activity
                    temp_violation = violation
                
                elif temp_violation['activity'] == violation['activity'] and temp_violation['person'] == violation['person'] and temp_violation['zone'] == violation['zone']:
                    print("updating end time of temp")
                    temp_violation['end_time'] = violation['end_time']
                
                else:
                    for policy in temp_violation['policy_id']:
                        violations.append(
                            {
                                'activity': temp_violation['activity'],
                                'person_id': temp_violation['person'],
                                'policy_id': policy,
                                'zone': temp_violation['zone'],
                                'start_time': temp_violation['start_time'],
                                'end_time': temp_violation['end_time']
                            }
                        )
                    temp_violation = False
            else:
                prev_violation = False
                if temp_violation != False:
                    for policy in temp_violation['policy_id']:
                        violations.append(
                            {
                                'activity': temp_violation['activity'],
                                'person_id': temp_violation['person'],
                                'policy_id': policy,
                                'zone': temp_violation['zone'],
                                'start_time': temp_violation['start_time'],
                                'end_time': temp_violation['end_time']
                            }
                        )
                    temp_violation = False
            
            print("Temp Violation:")
            print(temp_violation)

            # if violation != False:
            #     if len(violations) > 0 and violations[-1]['pending'] == True and violations[-1]['activity'] == violation['activity'] and violations[-1]['person'] == violation['person'] and violations[-1]['zone'] == violation['zone'] and violations[-1]['policy_id'] == violation['policy_id']:
            #         violations[-1]['end_time'] = violation['end_time']
            #     else:
            #         violation['pending'] = True
            #         violations.append(violation)
            # elif len(violations) > 0:
            #     violations[-1]['pending'] = False

            input_id += 1
            frames_queue = []

        if time.time() - violation_time_check > 10:
            if len(violations) > 0  or temp_violation != False:
                violation_time_check = time.time()
                if temp_violation != False:
                    for policy in temp_violation['policy_id']:
                        violations.append(
                                    {
                                        'activity': temp_violation['activity'],
                                        'person_id': temp_violation['person'],
                                        'policy_id': policy,
                                        'zone': temp_violation['zone'],
                                        'start_time': temp_violation['start_time'],
                                        'end_time': temp_violation['end_time']
                                    }
                                )
                        temp_violation = False
                print("sending violations to database")  
                print(violations)  
                for x in violations:
                    # if x['activity'] == prev_violation['activity'] and x['person_id'] == prev_violation['person_id'] and x['zone'] == prev_violation['zone']:
                    #     viol = Violations.objects.last()
                    #     serializer = ViolationsSerializer(viol, data=x)
                    # else:
                    serializer = ViolationsSerializer(data = x)
                    if serializer.is_valid():
                        serializer.save()
                    prev_violation = x
                violations = []
        count+=1
        # cv2.putText(image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        # cv2_imshow(image)

    vidcap.release()
    # Destroy all the windows
    cv2.destroyAllWindows()
    print("Violations:")
    print(violations)
    return violations

# temp_data = [['smoking', 2 , 'A', '5:04' , 4]]
def check_policy(POLICIES,activity_data):
    duration_check = False
    related_policy = []

    # identifying the related policies
    for policy in POLICIES:
        if policy['activity'] == activity_data['activity']:
            related_policy.append(policy)
        if policy['duration'] != None:
            duration_check = True

    # calculating time duration
    # if activity_data[3] != None:
    #     for temp in temp_data:
    #         if temp[0] == activity_data[0] and temp[1] == activity_data[1] and activity_data[1] != 'unknown':
    #             flag = 1            
    # select * from Policy where Policy.activity = activity
    # policies = [id, activity, zone, time, allowed/disallowed]
    policy_violated = []
    for policy in related_policy:
        if (policy['zone'] == None and policy['isallowed'] == False) or (policy['zone'] == activity_data['zone'] and policy['isallowed'] == False) or (policy['zone'] != activity_data['zone'] and policy['isallowed'] == True):
            policy_violated.append(policy['id'])
        # if (policy['duration'] == None) or (activity_data['duration'] > policy['duration'] and policy['isallowed'] == False):
        

    if len(policy_violated) > 0:
        return {
            'policy_id': policy_violated,
            'person': activity_data['person'],
            'activity': policy['activity'],
            'zone': activity_data['zone'],
            'start_time': activity_data['time'],
            'end_time': activity_data['time']

        }
    return False

# violatios = system_loop(SEQUENCE_LENGTH)

# print(violatios)


# Commented out IPython magic to ensure Python compatibility.

# drive.mount('/content/drive')
# # Specify the height and width to which each video frame will be resized in our dataset.
# # IMAGE_HEIGHT , IMAGE_WIDTH = 250, 250
# # # Specify the number of frames of a video that will be fed to the model as one sequence.
# # SEQUENCE_LENGTH = 10
# seed_constant = 27
# np.random.seed(seed_constant)
# random.seed(seed_constant)
# tf.random.set_seed(seed_constant)
# # Reading names
# # <===============
# # ==========================ADD YOUR DRIVE CLIPPED FOLDERS PATH IN HERE ========================================>
# videos = {'TalkingOnPhone':os.listdir('drive/MyDrive/Dataset/TalkingOnPhone'),'HoldingGun-Shooting':os.listdir('drive/MyDrive/Dataset/HoldingGun-Shooting')}

# # IMAGE_HEIGHT , IMAGE_WIDTH = 250, 250

# image_height, image_width = 100, 100
# max_images_per_class = 15000

# def frames_extraction(video_path):
#     # Empty List declared to store video frames
#     frames_list = []
    
#     # Reading the Video File Using the VideoCapture
#     video_reader = cv2.VideoCapture(video_path)

#     # Iterating through Video Frames
#     while True:

#         # Reading a frame from the video file 
#         success, frame = video_reader.read() 

#         # If Video frame was not successfully read then break the loop
#         if not success:
#             break

#         # Resize the Frame to fixed Dimensions
#         resized_frame = cv2.resize(frame, (image_height, image_width))
        
#         # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
#         normalized_frame = resized_frame / 255
        
#         # Appending the normalized frame into the frames list
#         frames_list.append(normalized_frame)
    
#     # Closing the VideoCapture object and releasing all resources. 
#     video_reader.release()

#     # returning the frames list 
#     return frames_list

# def create_dataset():

#     # Declaring Empty Lists to store the features and labels values.
#     temp_features = [] 
#     features = []
#     labels = []
    
#     # Iterating through all the classes mentioned in the classes list

#     for classname,file_names in videos.items():
#         print(f'Extracting {classname} Frames')
#         for video in file_names:
#           # Construct the complete video path
#           video_file_path = f'drive/MyDrive/Dataset/{classname}/{video}'
#           print(f'processing {video_file_path}')

#           # Calling the frame_extraction method for every video file path
#           frames = frames_extraction(video_file_path)

#           # Appending the frames to a temporary list.
#           temp_features.extend(frames)
          
#         # Adding randomly selected frames to the features list
#         features.extend(random.sample(temp_features, max_images_per_class))

#         # Adding Fixed number of labels to the labels list
#         labels.extend([classname] * max_images_per_class)
        
#         # Emptying the temp_features list so it can be reused to store all frames of the next class.
#         temp_features.clear()

#     # Converting the features and labels lists to numpy arrays
#     features = np.asarray(features)
#     labels = np.array(labels)  

#     return features, labels

# features, labels = create_dataset()

# # Saving the dataset in drive for later use 
# np.save('drive/MyDrive/FYP/Features_Labels/features_64', features)
# np.save('drive/MyDrive/FYP/Features_Labels/labels_64', labels)
# # np.save('drive/MyDrive/FYP/Features_Labels/paths', video_files_paths)

# le = LabelEncoder()
# labels = le.fit_transform(labels)

# # Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
# one_hot_encoded_labels = to_categorical(labels)

# features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.2, shuffle = True, random_state = seed_constant)

# model_output_size = len(videos)

# # Let's create a function that will construct our model
# def create_model():

#     # We will use a Sequential model for model construction
#     model = Sequential()

#     # Defining The Model Architecture
#     model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', input_shape = (image_height, image_width, 3)))
#     model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))
#     model.add(BatchNormalization())
#     model.add(MaxPooling2D(pool_size = (2, 2)))
#     model.add(GlobalAveragePooling2D())
#     model.add(Dense(256, activation = 'relu'))
#     model.add(BatchNormalization())
#     model.add(Dense(model_output_size, activation = 'softmax'))

#     # Printing the models summary
#     model.summary()

#     return model


# # Calling the create_model method
# model = create_model()

# print("Model Created Successfully!")

# plot_model(model, to_file = 'model_structure_plot.png', show_shapes = True, show_layer_names = True)

# # Adding the Early Stopping Callback to the model which will continuously monitor the validation loss metric for every epoch.
# # If the models validation loss does not decrease after 15 consecutive epochs, the training will be stopped and the weight which reported the lowest validation loss will be retored in the model.
# early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)

# # Adding loss, optimizer and metrics values to the model.
# model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# # Start Training
# model_training_history = model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4 , shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])

# model_evaluation_history = model.evaluate(features_test, labels_test)

# # Creating a useful name for our model, incase you're saving multiple models (OPTIONAL)
# date_time_format = '%Y_%m_%d__%H_%M_%S'
# current_date_time_dt = dt.datetime.now()
# current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
# model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history
# model_name = f'drive/MyDrive/FYP/Features_Labels/Model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# # Saving your Model
# model.save(model_name)

# def plot_metric(metric_name_1, metric_name_2, plot_name):
#     # Get Metric values using metric names as identifiers
#     metric_value_1 = model_training_history.history[metric_name_1]
#     metric_value_2 = model_training_history.history[metric_name_2]

#     # Constructing a range object which will be used as time 
#     epochs = range(len(metric_value_1))
    
#     # Plotting the Graph
#     plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
#     plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)
    
#     # Adding title to the plot
#     plt.title(str(plot_name))

#     # Adding legend to the plot
#     plt.legend()

# plot_metric('loss', 'val_loss', 'Total Loss vs Total Validation Loss')

# plot_metric('accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')

# def predict_on_live_video(video_file_path, output_file_path, window_size):

#     # Initialize a Deque Object with a fixed size which will be used to implement moving/rolling average functionality.
#     predicted_labels_probabilities_deque = deque(maxlen = window_size)

#     # Reading the Video File using the VideoCapture Object
#     video_reader = cv2.VideoCapture(video_file_path)

#     # Getting the width and height of the video 
#     original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
#     original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

#     # Writing the Overlayed Video Files Using the VideoWriter Object
#     video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 60, (original_video_width, original_video_height))

#     while True: 

#         # Reading The Frame
#         status, frame = video_reader.read() 

#         if not status:
#             break

#         # Resize the Frame to fixed Dimensions
#         resized_frame = cv2.resize(frame, (image_height, image_width))
        
#         # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
#         normalized_frame = resized_frame / 255

#         # Passing the Image Normalized Frame to the model and receiving Predicted Probabilities.
#         predicted_labels_probabilities = model.predict(np.expand_dims(normalized_frame, axis = 0))[0]

#         # Appending predicted label probabilities to the deque object
#         predicted_labels_probabilities_deque.append(predicted_labels_probabilities)

#         # Assuring that the Deque is completely filled before starting the averaging process
#         if len(predicted_labels_probabilities_deque) == window_size:

#             # Converting Predicted Labels Probabilities Deque into Numpy array
#             predicted_labels_probabilities_np = np.array(predicted_labels_probabilities_deque)

#             # Calculating Average of Predicted Labels Probabilities Column Wise 
#             predicted_labels_probabilities_averaged = predicted_labels_probabilities_np.mean(axis = 0)

#             # Converting the predicted probabilities into labels by returning the index of the maximum value.
#             predicted_label = np.argmax(predicted_labels_probabilities_averaged)

#             # Accessing The Class Name using predicted label.
#             predicted_class_name = classes_list[predicted_label]
#             print(predicted_class_name)
          
#             # Overlaying Class Name Text Ontop of the Frame
#             cv2.putText(frame, predicted_class_name + np.array_str(predicted_labels_probabilities), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

#         # Writing The Frame
#         video_writer.write(frame)


#         # cv2.imshow('Predicted Frames', frame)

#         # key_pressed = cv2.waitKey(10)

#         # if key_pressed == ord('q'):
#         #     break

#     # cv2.destroyAllWindows()

    
#     # Closing the VideoCapture and VideoWriter objects and releasing all resources held by them. 
#     video_reader.release()
#     video_writer.release()

# window_size = 1
# classes_list = [x for x in videos.keys()]
# # Construting The Output YouTube Video Path
# filename = 'T1Gun'
# input_video_file_path = f'drive/MyDrive/Dataset/TestGunPhone/{filename}.MP4'
# output_video_file_path = f'drive/MyDrive/Prediction_results/{filename}_64.MP4'

# # Calling the predict_on_live_video method to start the Prediction.
# predict_on_live_video(input_video_file_path, output_video_file_path, window_size)

 
# Display the output video.
#VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()

# def frames_extraction(video_path):
#     '''
#     This function will extract the required frames from a video after resizing and normalizing them.
#     Args:
#         video_path: The path of the video in the disk, whose frames are to be extracted.
#     Returns:
#         frames_list: A list containing the resized and normalized frames of the video.
#     '''
#     # Declare a list to store video frames.
#     frames_list = []
#     # Read the Video File using the VideoCapture object.
#     video_reader = cv2.VideoCapture(video_path)
#     # Get the total number of frames in the video.
#     video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
#     # Calculate the the interval after which frames will be added to the list.
#     skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)
#     # Iterate through the Video Frames.
#     for frame_counter in range(SEQUENCE_LENGTH):
 
#         # Set the current frame position of the video.
#         video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)
 
#         # Reading the frame from the video. 
#         success, frame = video_reader.read() 
 
#         # Check if Video frame is not successfully read then break the loop
#         if not success:
#             break
 
#         # Resize the Frame to fixed height and width.
#         resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
#         # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
#         normalized_frame = resized_frame / 255
        
#         # Append the normalized frame into the frames list
#         frames_list.append(normalized_frame)
    
#     # Release the VideoCapture object. 
#     video_reader.release()
 
#     # Return the frames list.
#     return frames_list

# def frames_extraction_augment(video_path):
#     '''
#     This function will extract the required frames from a video after resizing and normalizing them.
#     Args:
#         video_path: The path of the video in the disk, whose frames are to be extracted.
#     Returns:
#         frames_list: A list containing the resized and normalized frames of the video.
#     '''
#     # Declare a list to store video frames.
#     frames_list = []
#     flipped_list = []
#     # Read the Video File using the VideoCapture object.
#     video_reader = cv2.VideoCapture(video_path)
#     # Get the total number of frames in the video.
#     video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
#     # Calculate the the interval after which frames will be added to the list.
#     skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)
#     # Iterate through the Video Frames.
#     for frame_counter in range(SEQUENCE_LENGTH):
 
#         # Set the current frame position of the video.
#         video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)
 
#         # Reading the frame from the video. 
#         success, frame = video_reader.read() 
 
#         # Check if Video frame is not successfully read then break the loop
#         if not success:
#             break
 
#         # Resize the Frame to fixed height and width.
#         resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
#         # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
#         normalized_frame = resized_frame / 255
#         flipped_frame = cv2.flip(normalized_frame,3) # this line vertically flips the frame
        
#         # Append the normalized frame into the frames list
#         frames_list.append(normalized_frame)
#         flipped_list.append(flipped_frame)
    
#     # Release the VideoCapture object. 
#     video_reader.release()
 
#     # Return the frames list.
#     return frames_list, flipped_list

# def create_dataset():
#     '''
#     This function will extract the data of the selected classes and create the required dataset.
#     Returns:
#         features:          A list containing the extracted frames of the videos.
#         labels:            A list containing the indexes of the classes associated with the videos.
#         video_files_paths: A list containing the paths of the videos in the disk.
#     '''

#     # Declared Empty Lists to store the features, labels and video file path values.
#     features = []
#     labels = []
#     video_files_paths = []
    
#     # Iterating through all the classes mentioned in the classes list
#     count = 0
#     for classname,file_names in videos.items():
#       print(f'Extracting {classname} Frames')
#       for video in file_names:       
        
#         count+=1
#         # Get the complete video path.
#         video_file_path = f'drive/MyDrive/Dataset/{classname}/{video}'
#         print(f'processing {video_file_path}')
#         # Extract the frames of the video file.
#         # frames = frames_extraction(video_file_path) # without augment
#         frames,flipped = frames_extraction_augment(video_file_path) # with augment

#         # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
#         # So ignore the vides having frames less than the SEQUENCE_LENGTH.
#         # if len(frames) == SEQUENCE_LENGTH: # without augment
#         if len(frames) == SEQUENCE_LENGTH and len(flipped) == SEQUENCE_LENGTH: # with augment
#             # Append the data to their repective lists.
#             features.append(frames)
#             features.append(flipped) # this line can be commented if we do not want to augment dataset
#             labels.append( classname)
#             labels.append( classname)
#             video_files_paths.append(video_file_path)

#     return features, labels, video_files_paths

#     # Converting the list to numpy arrays
#     features = np.asarray(features)
#     labels = np.array(labels)  
    
#     # Return the frames, class index, and video file path.
#     return features, labels, video_files_paths

# # Create the dataset.
# features, labels, video_files_paths = create_dataset()
# # print(features)
# # print(labels)

# print(features)

# # Saving the dataset in drive for later use 
# np.save('drive/MyDrive/FYP/Features_Labels/features_250_5_aug', features)
# np.save('drive/MyDrive/FYP/Features_Labels/labels_250_5_aug', labels)
# # np.save('drive/MyDrive/FYP/Features_Labels/paths', video_files_paths)

# # To load data from the saved np files
# features = np.load('drive/MyDrive/FYP/Features_Labels/features_250_5_aug.npy')
# labels = np.load('drive/MyDrive/FYP/Features_Labels/labels_250_5_aug.npy')
# # video_files_paths = np.load('drive/MyDrive/FYP/Features_Labels/paths.npy')

# labels = np.load('drive/MyDrive/FYP/Features_Labels/labels_200.npy')

# print(features.shape)

# print(features.shape, features.dtype)
# print(labels.shape, labels.dtype)

# np.array(features)

# # Label encode the labels
# le = LabelEncoder()
# labels = le.fit_transform(labels)

# # Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
# one_hot_encoded_labels = to_categorical(labels)
# # Split the Data into Train ( 80% ) and Test Set ( 20% ).
# features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.20, shuffle = True, random_state = seed_constant)

# np.save('drive/MyDrive/FYP/Features_Labels/X_train_filenames_200_20.npy', features_train)
# np.save('drive/MyDrive/FYP/Features_Labels/y_train_200_20.npy', labels_train)

# np.save('drive/MyDrive/FYP/Features_Labels/X_test_filenames_200_20.npy', features_test)
# np.save('drive/MyDrive/FYP/Features_Labels/y_test_200_20.npy', labels_test)

# features_train = np.load('drive/MyDrive/FYP/Features_Labels/X_train_filenames_250_50.npy')
# labels_train = np.load('drive/MyDrive/FYP/Features_Labels/y_train_250_50.npy')

# print(features_train.shape)
# print(labels_train.shape)

# # class My_Custom_Generator(keras.utils.Sequence) :
  
# #   def __init__(self, image_filenames, labels, batch_size) :
# #     self.image_filenames = image_filenames
# #     self.labels = labels
# #     self.batch_size = batch_size
    
    
# #   def __len__(self) :
# #     return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)
  
  
# #   def __getitem__(self, idx) :
# #     batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]
# #     batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]
    
# #     return np.array([
# #             resize(imread('/content/all_images/' + str(file_name)), (80, 80, 3))
# #                for file_name in batch_x])/255.0, np.array(batch_y)

# def create_model():
#     '''
#     This function will construct the required LRCN model.
#     Returns:
#         model: It is the required constructed LRCN model.
#     '''
 
#     # We will use a Sequential model for model construction.
#     model = Sequential()
    
#     # Define the Model Architecture.
#     ########################################################################################################################
    
#     model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),
#                               input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
#     model.add(TimeDistributed(MaxPooling2D((4, 4)))) 
#     model.add(TimeDistributed(Dropout(0.25)))
    
#     model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))
#     model.add(TimeDistributed(MaxPooling2D((4, 4))))
#     model.add(TimeDistributed(Dropout(0.25)))
    
#     model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
#     model.add(TimeDistributed(MaxPooling2D((2, 2))))
#     model.add(TimeDistributed(Dropout(0.25)))
    
#     model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
#     model.add(TimeDistributed(MaxPooling2D((2, 2))))
#     #model.add(TimeDistributed(Dropout(0.25)))
                                      
#     model.add(TimeDistributed(Flatten()))
                                      
#     model.add(LSTM(32))
                                      
#     model.add(Dense(len(videos), activation = 'softmax'))
 
#     ########################################################################################################################
 
#     # Display the models summary.
#     model.summary()
    
#     # Return the constructed LRCN model.
#     return model

# model = create_model()
 
# # Display the success message.
# print("Model Created Successfully!")

# # Plot the structure of the contructed model.
# plot_model(model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)

# # Create an Instance of Early Stopping Callback.
# early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# # Compile the model and specify loss function, optimizer and metrics to the model.
# model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# # Start training the model.
# model_training_history = model.fit(x = features_train, y = labels_train, epochs = 70, batch_size = 4 , shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])

# features_test = np.load('drive/MyDrive/FYP/Features_Labels/X_test_filenames_200.npy')
# labels_test = np.load('drive/MyDrive/FYP/Features_Labels/y_test_200.npy')

# # To test the model 
# model_evaluation_history = model.evaluate(features_test, labels_test)

# # Get the loss and accuracy from model_evaluation_history.
# model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# # Define the string date format.
# # Get the current Date and Time in a DateTime Object.
# # Convert the DateTime object to string according to the style mentioned in date_time_format string.
# date_time_format = '%Y_%m_%d__%H_%M_%S'
# current_date_time_dt = dt.datetime.now()
# current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# # Define a useful name for our model to make it easy for us while navigating through multiple saved models.
# model_file_name = f'drive/MyDrive/FYP/Features_Labels/LRCN_model___11_11_22___Accuracy_0.625__Loss__0.6375__250_5_aug.h5'

# # Save the Model.
# model.save(model_file_name)

# from tensorflow import keras
# model = keras.models.load_model("drive/MyDrive/FYP/Features_Labels/LRCN_model___11_11_22___Accuracy_0.625__Loss__0.6375__250_10_aug.h5")

# def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
#     '''
#     This function will plot the metrics passed to it in a graph.
#     Args:
#         model_training_history: A history object containing a record of training and validation 
#                                 loss values and metrics values at successive epochs
#         metric_name_1:          The name of the first metric that needs to be plotted in the graph.
#         metric_name_2:          The name of the second metric that needs to be plotted in the graph.
#         plot_name:              The title of the graph.
#     '''
    
#     # Get metric values using metric names as identifiers.
#     metric_value_1 = model_training_history.history[metric_name_1]
#     metric_value_2 = model_training_history.history[metric_name_2]
    
#     # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
#     epochs = range(len(metric_value_1))

#     # Plot the Graph.
#     plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
#     plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

#     # Add title to the plot.
#     plt.title(str(plot_name))

#     # Add legend to the plot.
#     plt.legend()

# # Visualize the training and validation loss metrices.
# plot_metric(model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')

# plot_metric(model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')

# def download_youtube_videos(youtube_video_url, output_directory):
#      '''
#     This function downloads the youtube video whose URL is passed to it as an argument.
#     Args:
#         youtube_video_url: URL of the video that is required to be downloaded.
#         output_directory:  The directory path to which the video needs to be stored after downloading.
#     Returns:
#         title: The title of the downloaded youtube video.
#     '''

#     # Create a video object which contains useful information about the video.
#     video = pafy.new(youtube_video_url)

#     # Retrieve the title of the video.
#     title = video.title

#     # Get the best available quality object for the video.
#     video_best = video.getbest()

#     # Construct the output file path.
#     output_file_path = f'{output_directory}/{title}.mp4'

#     # Download the youtube video at the best available quality and store it to the contructed path.
#     video_best.download(filepath = output_file_path, quiet = True)

#     # Return the video title.
#     return title

# !pip install pafy
# !pip install youtube-dl==2020.12.2

# import pafy

# # Make the Output directory if it does not exist
# test_videos_directory = 'drive/MyDrive/Colab Notebooks/test_videos'
# os.makedirs(test_videos_directory, exist_ok = True)

# # Download a YouTube Video.
# video_title = download_youtube_videos('https://www.youtube.com/watch?v=AsIcrWa1tdc', test_videos_directory)

# # Get the YouTube Video's path we just downloaded.
# input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'

# def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
#     '''
#     This function will perform action recognition on a video using the LRCN model.
#     Args:
#     video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
#     output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
#     SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
#     '''

#     # Initialize the VideoCapture object to read from the video file.
#     video_reader = cv2.VideoCapture(video_file_path)
 
#     # Get the width and height of the video.
#     original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
#     original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))
 
#     # # Initialize the VideoWriter Object to store the output video in the disk.
#     video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
#                                    video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
 
#     # # Declare a queue to store video frames.
#     frames_queue = deque(maxlen = SEQUENCE_LENGTH)
 
#     # Initialize a variable to store the predicted action being performed in the video.
#     predicted_class_name = ''
#     a_shape = (1, 2)  # 3 rows and 4 columns
#     predicted_labels_probabilities = np.empty(a_shape)
     
#     # Iterate until the video is accessed successfully.
#     while video_reader.isOpened():
 
#         # Read the frame.
#         ok, frame = video_reader.read() 
        
#         # Check if frame is not read properly then break the loop.
#         if not ok:
#             break
#         # Resize the Frame to fixed Dimensions.
 
#         resized_frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))
        
#         # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
#         normalized_frame = resized_frame / 255
 
#         # Appending the pre-processed frame into the frames list.
#         frames_queue.append(normalized_frame)
 
#         # Check if the number of frames in the queue are equal to the fixed sequence length.
#         if len(frames_queue) == SEQUENCE_LENGTH:
#             # print("erere")
#             # Pass the normalized frames to the model and get the predicted probabilities.
#             predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]
 
#             # Get the index of class with highest probability.
#             predicted_label = np.argmax(predicted_labels_probabilities)
 
#             # Get the class name using the retrieved index.
#             predicted_class_name = CLASSES_LIST[predicted_label]
#             print(predicted_class_name)
#         # Write predicted class name on top of the frame.
        
#         cv2.putText(frame, predicted_class_name+ np.array_str(predicted_labels_probabilities), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
 
#         # Write The frame into the disk using the VideoWriter Object.
#         video_writer.write(frame)
        
#     # Release the VideoCapture and VideoWriter objects.
#     video_reader.release()
#     video_writer.release()

# def predict_on_video_cam(SEQUENCE_LENGTH):
#     vidcap = cv2.VideoCapture(0);
#     count = 0
#     success = True
#     frames_queue = deque(maxlen = SEQUENCE_LENGTH)
#     while success:
#         success,image = vidcap.read()
#         if not success:
#             break
#         #cv2.imshow('frame', image)
#         if count%30 == 0 :
#             resized_frame = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))
#             normalized_frame = resized_frame / 255
#             frames_queue.append(normalized_frame)
#             if len(frames_queue) == SEQUENCE_LENGTH:
#                 # Pass the normalized frames to the model and get the predicted probabilities.
#                 predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]
#                 # Get the index of class with highest probability.
#                 predicted_label = np.argmax(predicted_labels_probabilities)
#                 # Get the class name using the retrieved index.
#                 predicted_class_name = CLASSES_LIST[predicted_label]
#         count+=1
#         print(predicted_class_name)
#         if cv2.waitKey(1) & 0xFF == ord('q'):
#             break
#     vidcap.release()
#     # Destroy all the windows
#     cv2.destroyAllWindows()

# # Construct the output video path.
# CLASSES_LIST = [x for x in videos.keys()]
# # test_videos_directory = 'drive/MyDrive/Colab Notebooks/test_videos/'
# # video_path = "videoplayback"
# filename = 'T4Gun'
# input_video_file_path = f'drive/MyDrive/Dataset/TestGunPhone/{filename}.MP4'
# output_video_file_path = f'drive/MyDrive/Prediction_results/{filename}_250_10_owpn.mp4'
# # drive/MyDrive/predict_videos/{filename}.MP4
# # Perform Action Recognition on the Test Video.
# predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)
 
# # Display the output video.
# #VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()

# from moviepy.editor import *
# import cv2
# import numpy as np
# from PIL import Image
# import glob
# import random
# from google.colab.patches import cv2_imshow


# img = cv2.imread('drive/MyDrive/predict_videos/junaid.jpeg')
# def fill(img, h, w):
#     img = cv2.resize(img, (h, w), cv2.INTER_CUBIC)
#     return img
# def vertical_shift(img, ratio=0.0):
#     if ratio > 1 or ratio < 0:
#         print('Value should be less than 1 and greater than 0')
#         return img
#     ratio = random.uniform(-ratio, ratio)
#     h, w = img.shape[:2]
#     to_shift = h*ratio
#     if ratio > 0:
#         img = img[:int(h-to_shift), :, :]
#     if ratio < 0:
#         img = img[int(-1*to_shift):, :, :]
#     img = fill(img, h, w)
#     return img

# flip = cv2.flip(img,3)
# cv2_imshow(flip)
# cv2.waitKey(0)
# cv2.destroyAllWindows()